# Sigmoid

$$ logit(P) = ln\frac{p}{1-p}=f(x)$$
$$\frac{p}{1-p}=e^{f(x)},\\p=e^{f(x)}(1-p),\\p=e^{f(x)}-pe^{f(x)},\\p(1+e^{f(x)})=e^{f(x)}$$
$$ p=\frac{e^{f(x)}}{1+e^{f(x)}}=\frac{1}{1+e^{-{f(x)}}} $$
- 위 함수를 로지스틱 시그모이드(Logistic sigmoid function)이라고 하며, 줄여서 시그모이드 함수(sgimoid function)이라고 한다.
- 여기서 $f(x)$에 회귀 분석과 같은 함수 식을 넣으면, 로지스틱 회귀 모델(Logistic Regression Function)이 된다.

$$ f(x) = w^Tx = w_0x_0 + w_1x_1 + \ ... +w_mx_m $$

## 2. Sigmoid Fucntion in `python`
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

## 3. 시그모이드 함수의 장점과 단점

### A. 장점
- 출력 값의 범위가 0~1 사이이며, 매우 매끄러운 곡선을 가지므로, 후술 할 경사하강법을 시행할 떄, 기울기가 급격하게 변해서 발산하는, 기울기 폭주(Gradient Exploding)가 발생하지 않는다.
- 분류는 0과 1로 나뉘며, 출력 값이 어느 값에 가까운지를 통해 어느 분류에 속하는지 쉽게 알 수 있다.

### B. 단점
- 입력값이 아무리 크더라도, 출력되는 값의 범위가 매우 좁기 때문에, 경사하강법 수행 시 범위가 너무 좁아, 0에 수렴하는 기울기 소실(Gradient vanishing)이 발생할 수 있다.

### B.1 기울기 소실 문제(Gradient vanishing)
- 시그모이드 함수는 아무리 큰 값이 들어온다 할지라도 0~1 사이의 값만 반환함으로, 값이 현저하게 줄어들게 된다.
- 또한, 출력 값의 중앙값이 0이 아닌 0.5이며, 모두 양수기 때문에 출력의 가중치 합이 입력의 가중치 합보다 커지게 된다.
- 이를 편향 이동(Bias Gradient)라 하고, 신호가 각 레이어를 통과할 때마다 분산이 계속 커지게 되어, 활성화 함수의 출력이 최댓값과 최솟값인 0과 1에 수렴하게 된다.
- 시그모이드 함수의 도함수는 $\sigma(1-\sigma)$ 인데, 도함수에 들어가는 함수의 값이 0이나 1에 가까울수록 당연히 출력되는 값이 0에 가까워지게 된다.
- 이로 인해 수렴되는 뉴런의 기울기(Graddient) 값이 0이 되고, 역전파 시 0이 곱해져서 기울기가 소멸(kill)되는 현상이 발생해버린다. 즉, 역전파가 진행될수록 아래 층(Layer)에 아무런 신호가 전달되지 않는 것이다.
- 이를 기울기 소실(Gradient Vanishing)이라 하며, RELU 함수가 등장하기 전까지인 1986년부터 2006년까지 해결되지 않은 문제다.

### B.2 학습 속도 저하 문제
- 시그모이드 함수의 출력값은 모두 양수기 때문에, 경사하강법을 진행할 때, 그 기울기가 모두 양수거나 음수가 된다. 이는 기울기 업데이트가 지그재그로 변동하는 결과를 가지고 오고, 학습 효율성을 감소시켜 학습에 더 많은 시간이 들어가게 된다.

### Summary
시그모이드 함수의 장단점을 간추려 보면, 출력값이 너무 작아 제대로 학습이 안되는데다가 시간도 많이 잡아먹는다는 소리다.

이는 출력층에서 시그모이드 함수를 사용하는 것은 상관 없으나, 아래롤 정보가 계속 흘러가는 은닉층에서는 시그모이드 함수를 활성화 함수로 사용해서는 안된다는 소리다.

- 은닉층(Hidden layer)은 입력층(시작), 출력층(끝) 사이에 있는 부분이다.
- 즉, 은닉층에는 앞서 말했던 선형 함수와 시그모이드 함수는 사용하지 않는 것이 좋다.
- 시그모이드 함수는 이진 분류를 하고자 하는 경우 출력층에서만 사용하는 것을 권고한다.
- 만약, 입력층에서 시그모이드 함수를 쓰고자 한다면, 이의 발전형인 하이퍼볼릭 탄젠트 함수를 사용하는 것을 추천한다.

## Reference
[1] https://gooopy.tistory.com/52